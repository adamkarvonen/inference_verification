{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and dataset...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Environment setup\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "DATASET_NAME = \"lmsys/lmsys-chat-1m\"\n",
    "CTX_LEN = 1024\n",
    "MAX_DECODE_TOKENS = 512\n",
    "# MAX_DECODE_TOKENS = 32\n",
    "N_SAMPLES = 150\n",
    "DTYPE = \"bfloat16\"\n",
    "# OUTPUT_DIR = Path(f\"activations_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "OUTPUT_DIR = Path(\"activations\")\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Activation saving hook\n",
    "temp_saved_activations = []\n",
    "\n",
    "\n",
    "def activation_saving_hook(module, input, output):\n",
    "    temp_saved_activations.append(output[0].detach().clone())\n",
    "\n",
    "\n",
    "def concatenate_activations(activations_list):\n",
    "    \"\"\"Concatenate list of activation tensors along sequence dimension.\"\"\"\n",
    "    concatenated = []\n",
    "    for sample_activations in activations_list:\n",
    "        concat_tensor = torch.cat(sample_activations, dim=0)\n",
    "        concatenated.append(concat_tensor)\n",
    "    return concatenated\n",
    "\n",
    "\n",
    "def save_data(data, filename):\n",
    "    \"\"\"Save data to disk using pickle.\"\"\"\n",
    "    filepath = OUTPUT_DIR / filename\n",
    "    with open(filepath, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "\n",
    "print(\"Loading tokenizer and dataset...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "ds = load_dataset(DATASET_NAME, split=\"train\")\n",
    "\n",
    "# Prepare prompts\n",
    "prompts = [i[\"conversation\"] for _, i in zip(range(N_SAMPLES), ds)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'how can identity protection services help protect me against identity theft', 'role': 'user'}, {'content': \"Identity protection services can help protect you against identity theft in several ways:\\n\\n1. Monitoring: Many identity protection services monitor your credit reports, public records, and other sources for signs of identity theft. If they detect any suspicious activity, they will alert you so you can take action.\\n2. Credit freeze: Some identity protection services can help you freeze your credit, which makes it more difficult for thieves to open new accounts in your name.\\n3. Identity theft insurance: Some identity protection services offer insurance that can help you recover financially if you become a victim of identity theft.\\n4. Assistance: Many identity protection services offer assistance if you become a victim of identity theft. They can help you file a police report, contact credit bureaus, and other steps to help you restore your identity.\\n\\nOverall, identity protection services can provide you with peace of mind and help you take proactive steps to protect your identity. However, it's important to note that no service can completely guarantee that you will never become a victim of identity theft. It's still important to take steps to protect your own identity, such as being cautious with personal information and regularly monitoring your credit reports.\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "how can identity protection services help protect me against identity theft<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Identity protection services can help protect you against identity theft in several ways:\n",
      "\n",
      "1. Monitoring: Many identity protection services monitor your credit reports, public records, and other sources for signs of identity theft. If they detect any suspicious activity, they will alert you so you can take action.\n",
      "2. Credit freeze: Some identity protection services can help you freeze your credit, which makes it more difficult for thieves to open new accounts in your name.\n",
      "3. Identity theft insurance: Some identity protection services offer insurance that can help you recover financially if you become a victim of identity theft.\n",
      "4. Assistance: Many identity protection services offer assistance if you become a victim of identity theft. They can help you file a police report, contact credit bureaus, and other steps to help you restore your identity.\n",
      "\n",
      "Overall, identity protection services can provide you with peace of mind and help you take proactive steps to protect your identity. However, it's important to note that no service can completely guarantee that you will never become a victim of identity theft. It's still important to take steps to protect your own identity, such as being cautious with personal information and regularly monitoring your credit reports.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    tokenizer.apply_chat_template(prompt, tokenize=False) for prompt in prompts\n",
    "]\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize inputs\n",
    "tokenized_inputs = tokenizer(\n",
    "    prompts,\n",
    "    padding=False,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=False,\n",
    "    truncation=True,\n",
    "    max_length=CTX_LEN,\n",
    ")\n",
    "prompt_token_ids = [input_ids for input_ids in tokenized_inputs[\"input_ids\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 106, 1645, 108, 1139, 798, 12852, 6919, 3545, 1707, 9051, 682, 2691, 12852, 37214, 107, 108, 106, 2516, 108, 22869, 6919, 3545, 798, 1707, 9051, 692, 2691, 12852, 37214, 575, 3757, 5742, 235292, 109, 235274, 235265, 29654, 235292, 9429, 12852, 6919, 3545, 8200, 861, 6927, 8134, 235269, 2294, 9126, 235269, 578, 1156, 8269, 604, 11704, 576, 12852, 37214, 235265, 1927, 984, 21422, 1089, 45637, 5640, 235269, 984, 877, 14838, 692, 712, 692, 798, 1987, 3105, 235265, 108, 235284, 235265, 14882, 35059, 235292, 4213, 12852, 6919, 3545, 798, 1707, 692, 35059, 861, 6927, 235269, 948, 3833, 665, 978, 5988, 604, 72731, 577, 2174, 888, 12210, 575, 861, 1503, 235265, 108, 235304, 235265, 39310, 37214, 9490, 235292, 4213, 12852, 6919, 3545, 3255, 9490, 674, 798, 1707, 692, 11885, 50578, 1013, 692, 3831, 476, 17015, 576, 12852, 37214, 235265, 108, 235310, 235265, 38570, 235292, 9429, 12852, 6919, 3545, 3255, 11217, 1013, 692, 3831, 476, 17015, 576, 12852, 37214, 235265, 2365, 798, 1707, 692, 2482, 476, 5877, 3484, 235269, 3764, 6927, 180216, 235269, 578, 1156, 7161, 577, 1707, 692, 9825, 861, 12852, 235265, 109, 23081, 235269, 12852, 6919, 3545, 798, 3658, 692, 675, 7124, 576, 3403, 578, 1707, 692, 1987, 79016, 7161, 577, 9051, 861, 12852, 235265, 4560, 235269, 665, 235303, 235256, 2845, 577, 4670, 674, 793, 2566, 798, 7322, 16919, 674, 692, 877, 2447, 3831, 476, 17015, 576, 12852, 37214, 235265, 1165, 235303, 235256, 2076, 2845, 577, 1987, 7161, 577, 9051, 861, 1997, 12852, 235269, 1582, 685, 1855, 56608, 675, 3749, 2113, 578, 16969, 14251, 861, 6927, 8134, 235265, 107, 108]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(prompt_token_ids[0])\n",
    "print(type(prompt_token_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72955, 5179, 235274, 235304, 235304, 235315, 235276, 235260, 235308, 235258, 108]\n",
      "[72955, 94661, 13311, 235321, 235276, 235308, 235274, 235250, 235308, 235251, 108]\n"
     ]
    }
   ],
   "source": [
    "id_1 = \"Conversation ak13390c5d8\"\n",
    "id_2 = \"Conversation zxcf8051a5t\"\n",
    "\n",
    "id_tokens_1 = tokenizer.encode(id_1, add_special_tokens=False, return_tensors=None)\n",
    "id_tokens_2 = tokenizer.encode(id_2, add_special_tokens=False, return_tensors=None)\n",
    "\n",
    "min_length = min(len(id_tokens_1), len(id_tokens_2))\n",
    "\n",
    "id_tokens_1 = id_tokens_1[:min_length] + tokenizer.encode(\"\\n\", add_special_tokens=False)\n",
    "id_tokens_2 = id_tokens_2[:min_length] + tokenizer.encode(\"\\n\", add_special_tokens=False)\n",
    "\n",
    "print(id_tokens_1)\n",
    "print(id_tokens_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 106, 1645, 108, 1139, 798, 12852, 6919, 3545, 1707, 9051, 682, 2691, 12852, 37214, 107, 108, 106, 2516, 108, 22869, 6919, 3545, 798, 1707, 9051, 692, 2691, 12852, 37214, 575, 3757, 5742, 235292, 109, 235274, 235265, 29654, 235292, 9429, 12852, 6919, 3545, 8200, 861, 6927, 8134, 235269, 2294, 9126, 235269, 578, 1156, 8269, 604, 11704, 576, 12852, 37214, 235265, 1927, 984, 21422, 1089, 45637, 5640, 235269, 984, 877, 14838, 692, 712, 692, 798, 1987, 3105, 235265, 108, 235284, 235265, 14882, 35059, 235292, 4213, 12852, 6919, 3545, 798, 1707, 692, 35059, 861, 6927, 235269, 948, 3833, 665, 978, 5988, 604, 72731, 577, 2174, 888, 12210, 575, 861, 1503, 235265, 108, 235304, 235265, 39310, 37214, 9490, 235292, 4213, 12852, 6919, 3545, 3255, 9490, 674, 798, 1707, 692, 11885, 50578, 1013, 692, 3831, 476, 17015, 576, 12852, 37214, 235265, 108, 235310, 235265, 38570, 235292, 9429, 12852, 6919, 3545, 3255, 11217, 1013, 692, 3831, 476, 17015, 576, 12852, 37214, 235265, 2365, 798, 1707, 692, 2482, 476, 5877, 3484, 235269, 3764, 6927, 180216, 235269, 578, 1156, 7161, 577, 1707, 692, 9825, 861, 12852, 235265, 109, 23081, 235269, 12852, 6919, 3545, 798, 3658, 692, 675, 7124, 576, 3403, 578, 1707, 692, 1987, 79016, 7161, 577, 9051, 861, 12852, 235265, 4560, 235269, 665, 235303, 235256, 2845, 577, 4670, 674, 793, 2566, 798, 7322, 16919, 674, 692, 877, 2447, 3831, 476, 17015, 576, 12852, 37214, 235265, 1165, 235303, 235256, 2076, 2845, 577, 1987, 7161, 577, 9051, 861, 1997, 12852, 235269, 1582, 685, 1855, 56608, 675, 3749, 2113, 578, 16969, 14251, 861, 6927, 8134, 235265, 107, 108]\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple\n",
    "INSERT_POS = 4\n",
    "\n",
    "def insert_id_tokens(\n",
    "    prompts: List[List[int]],\n",
    "    id_tokens: List[int],\n",
    "    insert_pos: int = INSERT_POS,\n",
    "    max_len: int | None = None,\n",
    ") -> List[List[int]]:\n",
    "    \"\"\"\n",
    "    Return a NEW list of prompt token-lists with `id_tokens` spliced in.\n",
    "\n",
    "    Also returns a list recording the original prompt lengths; this makes\n",
    "    it easy to enforce the same total length when you swap IDs later.\n",
    "    \"\"\"\n",
    "    out: List[List[int]] = []\n",
    "\n",
    "    for ids in prompts:\n",
    "        new_ids = ids[:insert_pos] + id_tokens + ids[insert_pos:]\n",
    "\n",
    "        # Optional: trim the tail to respect ctx length\n",
    "        if max_len is not None and len(new_ids) > max_len:\n",
    "            new_ids = new_ids[:max_len]\n",
    "\n",
    "        out.append(new_ids)\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<bos>', '<start_of_turn>', 'user', '\\n', 'how', ' can', ' identity', ' protection', ' services', ' help', ' protect', ' me', ' against', ' identity', ' theft', '<end_of_turn>', '\\n', '<start_of_turn>', 'model', '\\n', 'Identity', ' protection', ' services', ' can', ' help', ' protect', ' you', ' against', ' identity', ' theft', ' in', ' several', ' ways', ':', '\\n\\n', '1', '.', ' Monitoring', ':', ' Many', ' identity', ' protection', ' services', ' monitor', ' your', ' credit', ' reports', ',', ' public', ' records', ',', ' and', ' other', ' sources', ' for', ' signs', ' of', ' identity', ' theft', '.', ' If', ' they', ' detect', ' any', ' suspicious', ' activity', ',', ' they', ' will', ' alert', ' you', ' so', ' you', ' can', ' take', ' action', '.', '\\n', '2', '.', ' Credit', ' freeze', ':', ' Some', ' identity', ' protection', ' services', ' can', ' help', ' you', ' freeze', ' your', ' credit', ',', ' which', ' makes', ' it', ' more', ' difficult', ' for', ' thieves', ' to', ' open', ' new', ' accounts', ' in', ' your', ' name', '.', '\\n', '3', '.', ' Identity', ' theft', ' insurance', ':', ' Some', ' identity', ' protection', ' services', ' offer', ' insurance', ' that', ' can', ' help', ' you', ' recover', ' financially', ' if', ' you', ' become', ' a', ' victim', ' of', ' identity', ' theft', '.', '\\n', '4', '.', ' Assistance', ':', ' Many', ' identity', ' protection', ' services', ' offer', ' assistance', ' if', ' you', ' become', ' a', ' victim', ' of', ' identity', ' theft', '.', ' They', ' can', ' help', ' you', ' file', ' a', ' police', ' report', ',', ' contact', ' credit', ' bureaus', ',', ' and', ' other', ' steps', ' to', ' help', ' you', ' restore', ' your', ' identity', '.', '\\n\\n', 'Overall', ',', ' identity', ' protection', ' services', ' can', ' provide', ' you', ' with', ' peace', ' of', ' mind', ' and', ' help', ' you', ' take', ' proactive', ' steps', ' to', ' protect', ' your', ' identity', '.', ' However', ',', ' it', \"'\", 's', ' important', ' to', ' note', ' that', ' no', ' service', ' can', ' completely', ' guarantee', ' that', ' you', ' will', ' never', ' become', ' a', ' victim', ' of', ' identity', ' theft', '.', ' It', \"'\", 's', ' still', ' important', ' to', ' take', ' steps', ' to', ' protect', ' your', ' own', ' identity', ',', ' such', ' as', ' being', ' cautious', ' with', ' personal', ' information', ' and', ' regularly', ' monitoring', ' your', ' credit', ' reports', '.', '<end_of_turn>', '\\n']]\n"
     ]
    }
   ],
   "source": [
    "def _list_decode(x: torch.Tensor):\n",
    "    assert len(x.shape) == 1 or len(x.shape) == 2\n",
    "    # Convert to list of lists, even if x is 1D\n",
    "    if len(x.shape) == 1:\n",
    "        x = x.unsqueeze(0)  # Make it 2D for consistent handling\n",
    "\n",
    "    # Convert tensor to list of list of ints\n",
    "    token_ids = x.tolist()\n",
    "    \n",
    "    # Convert token ids to token strings\n",
    "    return [tokenizer.batch_decode(seq, skip_special_tokens=False) for seq in token_ids]\n",
    "\n",
    "print(_list_decode(torch.tensor(prompt_token_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
