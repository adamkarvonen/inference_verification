{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/inference_verification/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-11 02:36:12 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 02:36:13,839\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['VLLM_USE_V1'] = '0'\n",
    "os.environ[\"NVTE_ALLOW_NONDETERMINISTIC_ALGO\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "import vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.model_executor import SamplingMetadata\n",
    "from vllm.model_executor.layers.logits_processor import _prune_hidden_states\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import inference_verification.data_utils as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2-2b-it\"\n",
    "dtype = \"bfloat16\"\n",
    "dataset_name = \"lmsys/lmsys-chat-1m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "ctx_len = 1024\n",
    "\n",
    "max_decode_tokens = 1024\n",
    "max_decode_tokens = 32\n",
    "n_samples = 10\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, ignore_eos=True, max_tokens=max_decode_tokens + 1)\n",
    "sampling_params = SamplingParams(temperature=0.0, ignore_eos=True, max_tokens=max_decode_tokens+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "1024\n",
      "359\n",
      "165\n",
      "210\n",
      "220\n",
      "320\n",
      "388\n",
      "701\n",
      "704\n",
      "<bos><start_of_turn>user\n",
      "how can identity protection services help protect me against identity theft<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Identity protection services can help protect you against identity theft in several ways:\n",
      "\n",
      "1. Monitoring: Many identity protection services monitor your credit reports, public records, and other sources for signs of identity theft. If they detect any suspicious activity, they will alert you so you can take action.\n",
      "2. Credit freeze: Some identity protection services can help you freeze your credit, which makes it more difficult for thieves to open new accounts in your name.\n",
      "3. Identity theft insurance: Some identity protection services offer insurance that can help you recover financially if you become a victim of identity theft.\n",
      "4. Assistance: Many identity protection services offer assistance if you become a victim of identity theft. They can help you file a police report, contact credit bureaus, and other steps to help you restore your identity.\n",
      "\n",
      "Overall, identity protection services can provide you with peace of mind and help you take proactive steps to protect your identity. However, it's important to note that no service can completely guarantee that you will never become a victim of identity theft. It's still important to take steps to protect your own identity, such as being cautious with personal information and regularly monitoring your credit reports.<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds = load_dataset(dataset_name, split=\"train\")\n",
    "prompts = [i['conversation'] for _, i in zip(range(n_samples), ds)]\n",
    "\n",
    "prompts = [tokenizer.apply_chat_template(prompt, tokenize=False) for prompt in prompts]\n",
    "\n",
    "tokenized_inputs = tokenizer(prompts, padding=False, return_tensors=None, add_special_tokens=False, truncation=True, max_length=ctx_len)\n",
    "prompt_token_ids = [input_ids for input_ids in tokenized_inputs[\"input_ids\"]]\n",
    "\n",
    "for i in range(len(prompt_token_ids)):\n",
    "    print(len(prompt_token_ids[i]))\n",
    "\n",
    "\n",
    "\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-11 02:36:22 [config.py:717] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-11 02:36:22 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='google/gemma-2-2b-it', speculative_config=None, tokenizer='google/gemma-2-2b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=google/gemma-2-2b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[],\"max_capture_size\":0}, use_cached_outputs=False, \n",
      "INFO 06-11 02:36:23 [cuda.py:292] Using Flash Attention backend.\n",
      "INFO 06-11 02:36:24 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 06-11 02:36:24 [model_runner.py:1108] Starting to load model google/gemma-2-2b-it...\n",
      "INFO 06-11 02:36:24 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.06it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-11 02:36:25 [loader.py:458] Loading weights took 1.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-11 02:36:25 [model_runner.py:1140] Model loading took 4.9000 GiB and 1.404967 seconds\n",
      "INFO 06-11 02:36:27 [worker.py:287] Memory profiling takes 1.18 seconds\n",
      "INFO 06-11 02:36:27 [worker.py:287] the current vLLM instance can use total_gpu_memory (23.58GiB) x gpu_memory_utilization (0.90) = 21.23GiB\n",
      "INFO 06-11 02:36:27 [worker.py:287] model weights take 4.90GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.34GiB; the rest of the memory reserved for KV Cache is 13.93GiB.\n",
      "INFO 06-11 02:36:27 [executor_base.py:112] # cuda blocks: 8779, # CPU blocks: 2520\n",
      "INFO 06-11 02:36:27 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 68.59x\n",
      "INFO 06-11 02:36:31 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 5.17 seconds\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=1,\n",
    "    max_model_len=ctx_len*2,\n",
    "    enforce_eager=True,\n",
    "    dtype=dtype,\n",
    "    disable_async_output_proc=True,\n",
    ")\n",
    "model = llm.llm_engine.model_executor.driver_worker.model_runner.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_saved_activations = []\n",
    "def activation_saving_hook(module, input, output):\n",
    "    temp_saved_activations.append(output[0].detach().clone())\n",
    "\n",
    "\n",
    "\n",
    "# def toploc_cache_hook(_, inputs: tuple, topk_cache: list):\n",
    "#     \"\"\"\n",
    "#     Pre-hook to get final hidden states of a model forward pass and add it to\n",
    "#     the TOPLOC cache based on sampling metadata.\n",
    "\n",
    "\n",
    "#     Args:\n",
    "#         _ (): Unused argument, required for compatibility with hook function signature.\n",
    "#         input (tuple): A tuple containing the input data. The first element is expected to be the hidden states of the model, and the second element is expected to be the sampling metadata.\n",
    "#         toploc_cache (TopLocCache): The TopLocCache instance to which the processed data will be added.\n",
    "#     \"\"\"\n",
    "#     print(len(inputs))\n",
    "#     print(inputs[0].shape)\n",
    "#     print(inputs[1].shape)\n",
    "#     # Get hidden states and sampling metadata from inputs\n",
    "#     hidden_states, sampling_metadata = inputs[1], inputs[2]\n",
    "#     assert isinstance(hidden_states, torch.Tensor)\n",
    "#     assert isinstance(sampling_metadata, SamplingMetadata)\n",
    "\n",
    "#     # This check is true only for prefills\n",
    "#     if max(sampling_metadata.selected_token_indices) > len(sampling_metadata.seq_groups):\n",
    "#         return\n",
    "\n",
    "#     # This pruning is required when cuda graph padding is enabled.\n",
    "#     hidden_states = _prune_hidden_states(hidden_states, sampling_metadata)\n",
    "#     if len(sampling_metadata.seq_groups) != hidden_states.shape[0]:\n",
    "#         raise ValueError(f\"Lengths dont match: {len(sampling_metadata.seq_groups)} {hidden_states.shape}\")\n",
    "\n",
    "#     # Get seq_ids from seq_groups\n",
    "#     seq_ids = [seq_group.seq_ids[0] for seq_group in sampling_metadata.seq_groups]\n",
    "\n",
    "#     # Add hidden states to TOPLOC cache with corresponding seq_ids\n",
    "#     topk_cache.append((hidden_states, seq_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating activations:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating activations: 100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "saved_activations_handle = model.model.norm.register_forward_hook(activation_saving_hook)\n",
    "# saved_activations_handle = model.model.norm.register_forward_hook(toploc_cache_hook)\n",
    "\n",
    "temp_saved_activations = []\n",
    "orig_activations = []\n",
    "\n",
    "prefill_tokens = []\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, ignore_eos=True, max_tokens=max_decode_tokens+1)\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(len(prompt_token_ids)), desc=\"Generating activations\"):\n",
    "        outputs = llm.generate(prompt_token_ids=prompt_token_ids[i], sampling_params=sampling_params, use_tqdm=False)\n",
    "        orig_activations.append(temp_saved_activations)\n",
    "        temp_saved_activations = []\n",
    "        # print(outputs)\n",
    "        # print(torch.tensor(outputs[0].outputs[0].token_ids).shape)\n",
    "        prefill_tokens.append(prompt_token_ids[i] + list(outputs[0].outputs[0].token_ids))\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    saved_activations_handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_activations(orig_activations: list[list[torch.Tensor]]) -> list[torch.Tensor]:\n",
    "    concatenated = []\n",
    "    for sample_activations in orig_activations:\n",
    "        # Concatenate all activations along the sequence dimension (dim=0)\n",
    "        concat_tensor = torch.cat(sample_activations, dim=0)\n",
    "        concatenated.append(concat_tensor)\n",
    "    \n",
    "    return concatenated\n",
    "\n",
    "orig_activations = concatenate_activations(orig_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292 291 259\n",
      "33\n",
      "1057 1056 1024\n",
      "33\n",
      "392 391 359\n",
      "33\n",
      "198 197 165\n",
      "33\n",
      "243 242 210\n",
      "33\n",
      "253 252 220\n",
      "33\n",
      "353 352 320\n",
      "33\n",
      "421 420 388\n",
      "33\n",
      "734 733 701\n",
      "33\n",
      "737 736 704\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prefill_tokens)):\n",
    "    print(len(prefill_tokens[i]), len(orig_activations[i]), len(prompt_token_ids[i]))\n",
    "    print(len(prefill_tokens[i]) - len(prompt_token_ids[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating activations:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating activations: 100%|██████████| 10/10 [00:00<00:00, 25.68it/s]\n"
     ]
    }
   ],
   "source": [
    "saved_activations_handle = model.model.norm.register_forward_hook(activation_saving_hook)\n",
    "\n",
    "temp_saved_activations = []\n",
    "prefill_activations = []\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, ignore_eos=True, max_tokens=1)\n",
    "\n",
    "try:\n",
    "    for i in tqdm(range(len(prefill_tokens)), desc=\"Generating activations\"):\n",
    "        outputs = llm.generate(prompt_token_ids=prefill_tokens[i], sampling_params=sampling_params, use_tqdm=False)\n",
    "        # remove the list, -1 because that wasn't in orig_activations\n",
    "        prefill_activations.append(temp_saved_activations[0][:-1])\n",
    "        temp_saved_activations = []\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    saved_activations_handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(orig_activations[0][0].shape)\n",
    "# print(orig_activations[0][1].shape)\n",
    "# print(orig_activations[0][2].shape)\n",
    "# print(len(orig_activations[0]))\n",
    "# print(len(orig_activations))\n",
    "\n",
    "# for i in range(len(orig_activations[0])):\n",
    "#     print(orig_activations[0][i].shape, i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([291, 2304])\n",
      "torch.Size([291, 2304])\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(prefill_activations[0].shape)\n",
    "print(orig_activations[0].shape)\n",
    "print(len(prefill_activations))\n",
    "print(len(orig_activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA_scatter__src)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 103\u001b[0m\n\u001b[1;32m    100\u001b[0m orig_compressed \u001b[38;5;241m=\u001b[39m compress_activations(torch\u001b[38;5;241m.\u001b[39mcat(orig_activations, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Compute similarities (use batch version for efficiency)\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_euclidean_similarity_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefill_compressed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_compressed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarities\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin similarity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimilarities\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 82\u001b[0m, in \u001b[0;36mcompute_euclidean_similarity_batch\u001b[0;34m(compressed1, compressed2, hidden_dim)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Use scatter to fill in values\u001b[39;00m\n\u001b[1;32m     81\u001b[0m batch_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(batch_size)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, k)\n\u001b[0;32m---> 82\u001b[0m \u001b[43msparse1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompressed1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompressed1\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m sparse2\u001b[38;5;241m.\u001b[39mscatter_(\u001b[38;5;241m1\u001b[39m, compressed2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindices\u001b[39m\u001b[38;5;124m'\u001b[39m], compressed2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Compute Euclidean distances for all samples at once\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method wrapper_CUDA_scatter__src)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compress_activations(activations, k=100):\n",
    "    \"\"\"\n",
    "    Compress activations by keeping only the top-k absolute values and their indices.\n",
    "    \n",
    "    Args:\n",
    "        activations: torch tensor of shape (num_samples, hidden_dim)\n",
    "        k: number of top values to keep per sample\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'values' and 'indices' tensors, both of shape (num_samples, k)\n",
    "    \"\"\"\n",
    "    # Get top-k values by absolute magnitude\n",
    "    abs_activations = torch.abs(activations)\n",
    "    top_k_values, top_k_indices = torch.topk(abs_activations, k=k, dim=1)\n",
    "    \n",
    "    # Get the actual values (with sign preserved) using gather\n",
    "    actual_values = torch.gather(activations, 1, top_k_indices).to(device=activations.device)\n",
    "    \n",
    "    return {\n",
    "        'values': actual_values,\n",
    "        'indices': top_k_indices\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_euclidean_similarity(compressed1, compressed2):\n",
    "    \"\"\"\n",
    "    Compute Euclidean-based similarity between two compressed activation representations.\n",
    "    \n",
    "    Args:\n",
    "        compressed1, compressed2: dicts with 'values' and 'indices' tensors\n",
    "    \n",
    "    Returns:\n",
    "        similarity scores for each sample (higher is more similar)\n",
    "    \"\"\"\n",
    "    batch_size = compressed1['values'].shape[0]\n",
    "    hidden_dim = compressed1['indices'].max().item() + 1  # Assuming indices are 0-based\n",
    "    \n",
    "    similarities = torch.zeros(batch_size)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Create sparse representations\n",
    "        sparse1 = torch.zeros(hidden_dim)\n",
    "        sparse2 = torch.zeros(hidden_dim)\n",
    "        \n",
    "        # Fill in the values at their respective indices\n",
    "        sparse1.scatter_(0, compressed1['indices'][i], compressed1['values'][i])\n",
    "        sparse2.scatter_(0, compressed2['indices'][i], compressed2['values'][i])\n",
    "        \n",
    "        # Compute Euclidean distance and convert to similarity\n",
    "        distance = torch.norm(sparse1 - sparse2, p=2)\n",
    "        similarities[i] = 1 / (1 + distance)  # Convert distance to similarity\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Alternative: Batch-efficient version\n",
    "def compute_euclidean_similarity_batch(compressed1, compressed2, hidden_dim=None):\n",
    "    \"\"\"\n",
    "    Batch-efficient version of Euclidean similarity computation.\n",
    "    \n",
    "    Args:\n",
    "        compressed1, compressed2: dicts with 'values' and 'indices' tensors\n",
    "        hidden_dim: dimension of the hidden layer (if None, inferred from indices)\n",
    "    \n",
    "    Returns:\n",
    "        similarity scores for each sample\n",
    "    \"\"\"\n",
    "    batch_size, k = compressed1['values'].shape\n",
    "    \n",
    "    if hidden_dim is None:\n",
    "        hidden_dim = max(compressed1['indices'].max().item(), \n",
    "                         compressed2['indices'].max().item()) + 1\n",
    "    \n",
    "    # Create sparse representations for the entire batch\n",
    "    sparse1 = torch.zeros(batch_size, hidden_dim)\n",
    "    sparse2 = torch.zeros(batch_size, hidden_dim)\n",
    "    \n",
    "    # Use scatter to fill in values\n",
    "    batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, k)\n",
    "    sparse1.scatter_(1, compressed1['indices'], compressed1['values'])\n",
    "    sparse2.scatter_(1, compressed2['indices'], compressed2['values'])\n",
    "    \n",
    "    # Compute Euclidean distances for all samples at once\n",
    "    distances = torch.norm(sparse1 - sparse2, p=2, dim=1)\n",
    "    similarities = 1 / (1 + distances)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming your activations are already torch tensors\n",
    "# If they're numpy arrays, convert them first:\n",
    "# prefill_activations = torch.from_numpy(prefill_activations)\n",
    "# orig_activations = torch.from_numpy(orig_activations)\n",
    "\n",
    "# Compress your activations\n",
    "prefill_compressed = compress_activations(torch.cat(prefill_activations, dim=0), k=100)\n",
    "orig_compressed = compress_activations(torch.cat(orig_activations, dim=0), k=100)\n",
    "\n",
    "# Compute similarities (use batch version for efficiency)\n",
    "similarities = compute_euclidean_similarity_batch(prefill_compressed, orig_compressed)\n",
    "\n",
    "print(f\"Average similarity: {similarities.mean():.4f}\")\n",
    "print(f\"Min similarity: {similarities.min():.4f}\")\n",
    "print(f\"Max similarity: {similarities.max():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
